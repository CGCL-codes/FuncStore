/**
 * Autogenerated by Thrift Compiler (0.18.1)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
#ifndef inference_service_H
#define inference_service_H

#include <thrift/TDispatchProcessor.h>
#include <thrift/async/TConcurrentClientSyncInfo.h>
#include <memory>
#include "model_service_types.h"

namespace hive { namespace predictor {

#ifdef _MSC_VER
  #pragma warning( push )
  #pragma warning (disable : 4250 ) //inheriting methods via dominance 
#endif

class inference_serviceIf {
 public:
  virtual ~inference_serviceIf() {}
  virtual void inference(rpc_label_vec& _return, const std::string& app, const rpc_features_vec& features) = 0;
};

class inference_serviceIfFactory {
 public:
  typedef inference_serviceIf Handler;

  virtual ~inference_serviceIfFactory() {}

  virtual inference_serviceIf* getHandler(const ::apache::thrift::TConnectionInfo& connInfo) = 0;
  virtual void releaseHandler(inference_serviceIf* /* handler */) = 0;
  };

class inference_serviceIfSingletonFactory : virtual public inference_serviceIfFactory {
 public:
  inference_serviceIfSingletonFactory(const ::std::shared_ptr<inference_serviceIf>& iface) : iface_(iface) {}
  virtual ~inference_serviceIfSingletonFactory() {}

  virtual inference_serviceIf* getHandler(const ::apache::thrift::TConnectionInfo&) override {
    return iface_.get();
  }
  virtual void releaseHandler(inference_serviceIf* /* handler */) override {}

 protected:
  ::std::shared_ptr<inference_serviceIf> iface_;
};

class inference_serviceNull : virtual public inference_serviceIf {
 public:
  virtual ~inference_serviceNull() {}
  void inference(rpc_label_vec& /* _return */, const std::string& /* app */, const rpc_features_vec& /* features */) override {
    return;
  }
};

typedef struct _inference_service_inference_args__isset {
  _inference_service_inference_args__isset() : app(false), features(false) {}
  bool app :1;
  bool features :1;
} _inference_service_inference_args__isset;

class inference_service_inference_args {
 public:

  inference_service_inference_args(const inference_service_inference_args&);
  inference_service_inference_args& operator=(const inference_service_inference_args&);
  inference_service_inference_args() noexcept
                                   : app() {
  }

  virtual ~inference_service_inference_args() noexcept;
  std::string app;
  rpc_features_vec features;

  _inference_service_inference_args__isset __isset;

  void __set_app(const std::string& val);

  void __set_features(const rpc_features_vec& val);

  bool operator == (const inference_service_inference_args & rhs) const
  {
    if (!(app == rhs.app))
      return false;
    if (!(features == rhs.features))
      return false;
    return true;
  }
  bool operator != (const inference_service_inference_args &rhs) const {
    return !(*this == rhs);
  }

  bool operator < (const inference_service_inference_args & ) const;

  template <class Protocol_>
  uint32_t read(Protocol_* iprot);
  template <class Protocol_>
  uint32_t write(Protocol_* oprot) const;

};


class inference_service_inference_pargs {
 public:


  virtual ~inference_service_inference_pargs() noexcept;
  const std::string* app;
  const rpc_features_vec* features;

  template <class Protocol_>
  uint32_t write(Protocol_* oprot) const;

};

typedef struct _inference_service_inference_result__isset {
  _inference_service_inference_result__isset() : success(false) {}
  bool success :1;
} _inference_service_inference_result__isset;

class inference_service_inference_result {
 public:

  inference_service_inference_result(const inference_service_inference_result&);
  inference_service_inference_result& operator=(const inference_service_inference_result&);
  inference_service_inference_result() noexcept {
  }

  virtual ~inference_service_inference_result() noexcept;
  rpc_label_vec success;

  _inference_service_inference_result__isset __isset;

  void __set_success(const rpc_label_vec& val);

  bool operator == (const inference_service_inference_result & rhs) const
  {
    if (!(success == rhs.success))
      return false;
    return true;
  }
  bool operator != (const inference_service_inference_result &rhs) const {
    return !(*this == rhs);
  }

  bool operator < (const inference_service_inference_result & ) const;

  template <class Protocol_>
  uint32_t read(Protocol_* iprot);
  template <class Protocol_>
  uint32_t write(Protocol_* oprot) const;

};

typedef struct _inference_service_inference_presult__isset {
  _inference_service_inference_presult__isset() : success(false) {}
  bool success :1;
} _inference_service_inference_presult__isset;

class inference_service_inference_presult {
 public:


  virtual ~inference_service_inference_presult() noexcept;
  rpc_label_vec* success;

  _inference_service_inference_presult__isset __isset;

  template <class Protocol_>
  uint32_t read(Protocol_* iprot);

};

template <class Protocol_>
class inference_serviceClientT : virtual public inference_serviceIf {
 public:
  inference_serviceClientT(std::shared_ptr< Protocol_> prot) {
    setProtocolT(prot);
  }
  inference_serviceClientT(std::shared_ptr< Protocol_> iprot, std::shared_ptr< Protocol_> oprot) {
    setProtocolT(iprot,oprot);
  }
 private:
  void setProtocolT(std::shared_ptr< Protocol_> prot) {
  setProtocolT(prot,prot);
  }
  void setProtocolT(std::shared_ptr< Protocol_> iprot, std::shared_ptr< Protocol_> oprot) {
    piprot_=iprot;
    poprot_=oprot;
    iprot_ = iprot.get();
    oprot_ = oprot.get();
  }
 public:
  std::shared_ptr< ::apache::thrift::protocol::TProtocol> getInputProtocol() {
    return this->piprot_;
  }
  std::shared_ptr< ::apache::thrift::protocol::TProtocol> getOutputProtocol() {
    return this->poprot_;
  }
  void inference(rpc_label_vec& _return, const std::string& app, const rpc_features_vec& features) override;
  void send_inference(const std::string& app, const rpc_features_vec& features);
  void recv_inference(rpc_label_vec& _return);
 protected:
  std::shared_ptr< Protocol_> piprot_;
  std::shared_ptr< Protocol_> poprot_;
  Protocol_* iprot_;
  Protocol_* oprot_;
};

typedef inference_serviceClientT< ::apache::thrift::protocol::TProtocol> inference_serviceClient;

template <class Protocol_>
class inference_serviceProcessorT : public ::apache::thrift::TDispatchProcessorT<Protocol_> {
 protected:
  ::std::shared_ptr<inference_serviceIf> iface_;
  virtual bool dispatchCall(::apache::thrift::protocol::TProtocol* iprot, ::apache::thrift::protocol::TProtocol* oprot, const std::string& fname, int32_t seqid, void* callContext) override;
  virtual bool dispatchCallTemplated(Protocol_* iprot, Protocol_* oprot, const std::string& fname, int32_t seqid, void* callContext);
 private:
  typedef  void (inference_serviceProcessorT::*ProcessFunction)(int32_t, ::apache::thrift::protocol::TProtocol*, ::apache::thrift::protocol::TProtocol*, void*);
  typedef void (inference_serviceProcessorT::*SpecializedProcessFunction)(int32_t, Protocol_*, Protocol_*, void*);
  struct ProcessFunctions {
    ProcessFunction generic;
    SpecializedProcessFunction specialized;
    ProcessFunctions(ProcessFunction g, SpecializedProcessFunction s) :
      generic(g),
      specialized(s) {}
    ProcessFunctions() : generic(nullptr), specialized(nullptr) {}
  };
  typedef std::map<std::string, ProcessFunctions> ProcessMap;
  ProcessMap processMap_;
  void process_inference(int32_t seqid, ::apache::thrift::protocol::TProtocol* iprot, ::apache::thrift::protocol::TProtocol* oprot, void* callContext);
  void process_inference(int32_t seqid, Protocol_* iprot, Protocol_* oprot, void* callContext);
 public:
  inference_serviceProcessorT(::std::shared_ptr<inference_serviceIf> iface) :
    iface_(iface) {
    processMap_["inference"] = ProcessFunctions(
      &inference_serviceProcessorT::process_inference,
      &inference_serviceProcessorT::process_inference);
  }

  virtual ~inference_serviceProcessorT() {}
};

typedef inference_serviceProcessorT< ::apache::thrift::protocol::TDummyProtocol > inference_serviceProcessor;

template <class Protocol_>
class inference_serviceProcessorFactoryT : public ::apache::thrift::TProcessorFactory {
 public:
  inference_serviceProcessorFactoryT(const ::std::shared_ptr< inference_serviceIfFactory >& handlerFactory) noexcept :
      handlerFactory_(handlerFactory) {}

  ::std::shared_ptr< ::apache::thrift::TProcessor > getProcessor(const ::apache::thrift::TConnectionInfo& connInfo) override;

 protected:
  ::std::shared_ptr< inference_serviceIfFactory > handlerFactory_;
};

typedef inference_serviceProcessorFactoryT< ::apache::thrift::protocol::TDummyProtocol > inference_serviceProcessorFactory;

class inference_serviceMultiface : virtual public inference_serviceIf {
 public:
  inference_serviceMultiface(std::vector<std::shared_ptr<inference_serviceIf> >& ifaces) : ifaces_(ifaces) {
  }
  virtual ~inference_serviceMultiface() {}
 protected:
  std::vector<std::shared_ptr<inference_serviceIf> > ifaces_;
  inference_serviceMultiface() {}
  void add(::std::shared_ptr<inference_serviceIf> iface) {
    ifaces_.push_back(iface);
  }
 public:
  void inference(rpc_label_vec& _return, const std::string& app, const rpc_features_vec& features) override {
    size_t sz = ifaces_.size();
    size_t i = 0;
    for (; i < (sz - 1); ++i) {
      ifaces_[i]->inference(_return, app, features);
    }
    ifaces_[i]->inference(_return, app, features);
    return;
  }

};

// The 'concurrent' client is a thread safe client that correctly handles
// out of order responses.  It is slower than the regular client, so should
// only be used when you need to share a connection among multiple threads
template <class Protocol_>
class inference_serviceConcurrentClientT : virtual public inference_serviceIf {
 public:
  inference_serviceConcurrentClientT(std::shared_ptr< Protocol_> prot, std::shared_ptr< ::apache::thrift::async::TConcurrentClientSyncInfo> sync) : sync_(sync)
{
    setProtocolT(prot);
  }
  inference_serviceConcurrentClientT(std::shared_ptr< Protocol_> iprot, std::shared_ptr< Protocol_> oprot, std::shared_ptr< ::apache::thrift::async::TConcurrentClientSyncInfo> sync) : sync_(sync)
{
    setProtocolT(iprot,oprot);
  }
 private:
  void setProtocolT(std::shared_ptr< Protocol_> prot) {
  setProtocolT(prot,prot);
  }
  void setProtocolT(std::shared_ptr< Protocol_> iprot, std::shared_ptr< Protocol_> oprot) {
    piprot_=iprot;
    poprot_=oprot;
    iprot_ = iprot.get();
    oprot_ = oprot.get();
  }
 public:
  std::shared_ptr< ::apache::thrift::protocol::TProtocol> getInputProtocol() {
    return this->piprot_;
  }
  std::shared_ptr< ::apache::thrift::protocol::TProtocol> getOutputProtocol() {
    return this->poprot_;
  }
  void inference(rpc_label_vec& _return, const std::string& app, const rpc_features_vec& features) override;
  int32_t send_inference(const std::string& app, const rpc_features_vec& features);
  void recv_inference(rpc_label_vec& _return, const int32_t seqid);
 protected:
  std::shared_ptr< Protocol_> piprot_;
  std::shared_ptr< Protocol_> poprot_;
  Protocol_* iprot_;
  Protocol_* oprot_;
  std::shared_ptr< ::apache::thrift::async::TConcurrentClientSyncInfo> sync_;
};

typedef inference_serviceConcurrentClientT< ::apache::thrift::protocol::TProtocol> inference_serviceConcurrentClient;

#ifdef _MSC_VER
  #pragma warning( pop )
#endif

}} // namespace

#include "inference_service.tcc"
#include "model_service_types.tcc"

#endif
